{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7206f0-719f-4d28-9145-2dacd37d9ec8",
   "metadata": {},
   "source": [
    "# Manual Live BusObservatory Data Lake Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c3d50-306e-41a4-9745-f55819392659",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_id = \"TEST_tfnsw_bus\"\n",
    "# system_id = \"TEST_njtransit_bus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767850d-08a5-4d3f-8504-7e2519f0e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_PROFILE\"] = \"remote_notebook_user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbedc48-817c-4914-8110-225ffc1de4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "block_size = str(1024 * 1024 * 128)\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.2')\n",
    "        .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.profile.ProfileCredentialsProvider')\n",
    "        .set(\"spark.hadoop.dfs.block.size\", block_size)\n",
    "        .set('spark.driver.port', '7077')\n",
    "        .set('spark.driver.cores', '2')\n",
    "        .set('spark.driver.memory', '4g')\n",
    "        .set('spark.executor.memory', '2g')\n",
    "       )\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('BusObservatory-manual-compaction')\n",
    "         .config(conf=conf)\n",
    "         .getOrCreate()\n",
    "        )\n",
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e50af-7e32-4f14-9da2-cb217da56906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO update for main data lake\n",
    "# bucket = “busobservatory”\n",
    "bucket = \"busobservatory-migration\"\n",
    "in_path = f\"s3a://{bucket}/{system_id}/*\"\n",
    "# out_path = f\"s3a://{bucket}/{system_id}-compacted/\"\n",
    "out_path = f\"s3a://{bucket}/{system_id}/compacted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081f8ce-c724-4d9d-9616-bf374ad4d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import math\n",
    "\n",
    "def get_folder_size(bucket, path):\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    total_size = 0\n",
    "    count_obj = 0\n",
    "    for obj in my_bucket.objects.filter(Prefix=path):\n",
    "        total_size = total_size + obj.size\n",
    "        count_obj = count_obj + 1\n",
    "\n",
    "    return total_size, count_obj\n",
    "\n",
    "def get_repartition_factor(dir_size):\n",
    "    # block_size = sc._jsc.hadoopConfiguration().get(\"dfs.blocksize\")\n",
    "    block_size = int(spark._jsc.hadoopConfiguration().get(\"dfs.blocksize\"))\n",
    "    return math.ceil(dir_size/block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dad16-0043-411a-aeca-e261bd8ea767",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_size, count_obj = get_folder_size(bucket, system_id)\n",
    "num_files = get_repartition_factor(dir_size)\n",
    "block_size = int(spark._jsc.hadoopConfiguration().get(\"dfs.blocksize\"))\n",
    "print(f'Compacting {dir_size/1024/1024:.1f} Mb from {count_obj} files into {num_files} files of {block_size/1024/1024:.1f} Mb each.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859af6f8-f734-4271-aeb4-ac33c9207db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df=spark.read.csv(\"<hdfs_directory>\").\\\n",
    "withColumn(\"filename\",input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc8096-b705-4a66-b8a6-0fecf284f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in_path and out_path are the same, this overwrites the data before it can read it all\n",
    "\n",
    "df = (spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(in_path)\n",
    "      .repartition(num_files)\n",
    "      .write\n",
    "      .option(\"dataChange\", \"false\")\n",
    "      .format(\"parquet\")\n",
    "      .mode(\"overwrite\")\n",
    "      .save(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4f84c-28ff-4a5e-a31e-18c5ee158c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_size, out_count_obj = get_folder_size(bucket, f'{system_id}-compacted')\n",
    "in_dir_size, in_count_obj = get_folder_size(bucket, system_id)\n",
    "num_files = get_repartition_factor(in_dir_size)\n",
    "block_size = int(spark._jsc.hadoopConfiguration().get(\"dfs.blocksize\"))\n",
    "print(f'Compaction complete: {out_dir_size/1024/1024:.1f} Mb in {out_count_obj-1} files is near optimal ({num_files} files of {block_size/1024/1024:.1f} Mb each.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19616beb-44af-4e2e-a039-ccf2dcee8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO delete files read in?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
