{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5710d7c0-c652-4b40-b8e0-56406786029a",
   "metadata": {},
   "source": [
    "# BusObservatory Compactor (no Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd3fc44-2fab-4277-a79f-afb73782e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change print statements to logging?\n",
    "#TODO convert to a lambda and test (will need a lot of memory)\n",
    "#TODO debug the pip dependencies for boto3 and s3fs (notebook on laptop used conda)\n",
    "#TODO put that lambda in a top-level folder in the project\n",
    "#TODO add the lambda to template.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8c55156-218d-443e-af87-b4fd1cad02eb",
   "metadata": {},
   "source": [
    "Reads all the files in each s3://{bucket}/incoming/{system_id} and compacts them to a single parquet in s3://{bucket}/{system_id}\n",
    "\n",
    "Need to run often to avoid really big files, as it doesn't use Spark and ergo doesn't repartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bff30e4-49c7-4bc8-b287-59116d24dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "region=\"us-east-1\"\n",
    "bucket=\"busobservatory-migration\"\n",
    "config_object_key = \"_bus_observatory_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663a398a-f76f-4ec0-8b6f-7999354138e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d0164e-728d-4047-89f0-19dee86b3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get config / list of systems\n",
    "def get_system_list(bucket, region,config_object_key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket, config_object_key)\n",
    "    config = json.load(obj.get()['Body'])\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c27600b-902e-4709-9bd1-db219a746881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_files_using_paginator(bucket,system_id):\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    response = paginator.paginate(Bucket=bucket, \n",
    "                                  PaginationConfig={\"PageSize\": 1000}, \n",
    "                                  Prefix=f\"incoming/{system_id}\"\n",
    "                                 )\n",
    "    object_list = []                 \n",
    "    for page in response:\n",
    "        # print(\"getting 1000 files from S3\")\n",
    "        files = page.get(\"Contents\")\n",
    "        # for file in files:\n",
    "        #     print(f\"file_name: {file['Key']}, size: {file['Size']}\")\n",
    "        # print(\"#\" * 10)\n",
    "        object_list.extend([f['Key'] for f in files])\n",
    "        \n",
    "    return object_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc68f0e0-009b-4857-b586-4cd9c435a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_list_of_objects_s3(bucket, object_list):\n",
    "    object_dict = [{\"Key\":f} for f in object_list]\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    response = s3_client.delete_objects(\n",
    "        Bucket=bucket,\n",
    "        Delete={\"Objects\": object_dict},\n",
    "    )\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca8fecd-071f-4992-b9f4-9696128f1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact a single system\n",
    "def compact_and_deliver_incoming(bucket, system_id):\n",
    "    \n",
    "\n",
    "    # get list of objects in incoming\n",
    "    object_list = list_s3_files_using_paginator(bucket,system_id)\n",
    "    \n",
    "    # for f in object_list:\n",
    "    #     print(f)\n",
    "    \n",
    "    # read each object_list file as parquet and append to a df\n",
    "    df = pd.concat(\n",
    "        pd.read_parquet(f's3://{bucket}/{object}')\n",
    "        for object in object_list\n",
    "    )\n",
    "    print(f'{system_id}: read {df.shape} dataframe from {len(object_list)} files.')\n",
    "    \n",
    "    # write the new df to the lake s3://{bucket}/{system_id}\n",
    "    timestamp = datetime.datetime.now().isoformat().replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "    outfile=f\"s3://{bucket}/{system_id}/{timestamp}.parquet\"\n",
    "    \n",
    "    #DEBUG create folder if its not there?\n",
    "    df.to_parquet(outfile, compression='snappy')\n",
    "    print(f'{system_id}: wrote {df.shape} dataframe to {outfile}')\n",
    "    \n",
    "    # delete all of the object_list files\n",
    "    delete_list_of_objects_s3(bucket, object_list)\n",
    "    print(f'{system_id}: deleted {len(object_list)} files from s3://{bucket}/incoming/{system_id}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb651600-e1ab-4c88-a02b-fcf662ec0a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_tfnsw_bus: read (86311, 21) dataframe from 403 files.\n",
      "TEST_tfnsw_bus: wrote (86311, 21) dataframe to s3://busobservatory-migration/TEST_tfnsw_bus/2022_08_30T16_02_03.443283.parquet\n",
      "TEST_tfnsw_bus: deleted 403 files from s3://busobservatory-migration/incoming/TEST_tfnsw_bus\n",
      "TEST_nyct_mta_bus_siri: read (730695, 23) dataframe from 400 files.\n",
      "TEST_nyct_mta_bus_siri: wrote (730695, 23) dataframe to s3://busobservatory-migration/TEST_nyct_mta_bus_siri/2022_08_30T16_06_32.253397.parquet\n",
      "TEST_nyct_mta_bus_siri: deleted 400 files from s3://busobservatory-migration/incoming/TEST_nyct_mta_bus_siri\n",
      "TEST_nyct_mta_bus_gtfsrt: read (842342, 11) dataframe from 339 files.\n",
      "TEST_nyct_mta_bus_gtfsrt: wrote (842342, 11) dataframe to s3://busobservatory-migration/TEST_nyct_mta_bus_gtfsrt/2022_08_30T16_10_18.256205.parquet\n",
      "TEST_nyct_mta_bus_gtfsrt: deleted 339 files from s3://busobservatory-migration/incoming/TEST_nyct_mta_bus_gtfsrt\n",
      "TEST_njtransit_bus: read (190901, 24) dataframe from 301 files.\n",
      "TEST_njtransit_bus: wrote (190901, 24) dataframe to s3://busobservatory-migration/TEST_njtransit_bus/2022_08_30T16_12_57.315947.parquet\n",
      "TEST_njtransit_bus: deleted 301 files from s3://busobservatory-migration/incoming/TEST_njtransit_bus\n"
     ]
    }
   ],
   "source": [
    "# get config\n",
    "bus_observatory_config = get_system_list(bucket,region,config_object_key)\n",
    "\n",
    "# iterate over systems\n",
    "for system_id, system_config in bus_observatory_config.items():\n",
    "    compact_and_deliver_incoming(bucket, system_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0ce98-df45-4a42-91ff-01634c46550a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
